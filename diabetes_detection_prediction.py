# -*- coding: utf-8 -*-
"""Diabetes Detection Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A6CQmLSIxfDDLRz4MiKHMMojtLViVvW4
"""

# Commented out IPython magic to ensure Python compatibility.
import pickle
import xgboost
import numpy as np
import pandas as pd
import seaborn as sns

from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score

import matplotlib.pyplot as plt
# %matplotlib inline

import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("diabetes.csv")
df.head()

"""FEATURE ENGINEERING"""

# dataset shape : number of records x number of features
print(df.shape)

df.info()

"""### Checking correlation between features"""

corr_mat = df.corr()
top_corr_features = corr_mat.index
plt.figure(figsize=(11, 11))
g = sns.heatmap(corr_mat[top_corr_features].corr(), annot=True, cmap="Blues")

# converting output label i.e. df[diabetes] from boolean to int.
df['Outcome'] = df['Outcome'].astype(int)
df.head()

diabetes_true_count = len(df.loc[df['Outcome'] == 1])
diabetes_false_count = len(df.loc[df['Outcome'] == 0])

print("Data having 1 as output: {}".format(diabetes_true_count))
print("Data having 0 as output: {}".format(diabetes_false_count))

"""### Train Test Split"""

all_features = ['Pregnancies', 
                 'Glucose', 
                 'BloodPressure', 
                 'SkinThickness', 
                 'Insulin',
                 'BMI', 
                 'DiabetesPedigreeFunction', 
                 'Age']

X = df[all_features]
y = df['Outcome']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""### Checking values which are 0"""

print("total number of rows : {0}".format(len(df)))
print("number of rows missing Glucose: {0}".format(len(df[df['Glucose'] == 0])))
print("number of rows missing BloodPressure: {0}".format(len(df[df['BloodPressure'] == 0])))
print("number of rows missing insulin: {0}".format(len(df[df['Insulin'] == 0])))
print("number of rows missing bmi: {0}".format(len(df[df['BMI'] == 0])))
print("number of rows missing DiabetesPedigreeFunction: {0}".format(len(df[df['DiabetesPedigreeFunction'] == 0])))
print("number of rows missing age: {0}".format(len(df[df['Age'] == 0])))
print("number of rows missing SkinThickness: {0}".format(len(df[df['SkinThickness'] == 0])))

"""### Imputing these missing/zero values"""

missing_values_imputer = SimpleImputer(missing_values=0, strategy='mean')

X_train = missing_values_imputer.fit_transform(X_train)
X_test = missing_values_imputer.fit_transform(X_test)

X_train.shape, X_test.shape

"""### Training"""

# using random forest classifier
rfc = RandomForestClassifier(random_state=10)
# Создание экземпляра класса случайного леса 

rfc.fit(X_train, y_train)
# Подгонка модели

# random forest classifier accuracy
y_preds = rfc.predict(X_test)
# Предсказание данных

print(f"Accuracy : {accuracy_score(y_test, y_preds)*100}%")
# Показатель точности

"""### Random Forest Feature Importance"""

f_importance = pd.DataFrame(rfc.feature_importances_*100,index=all_features,columns=['Importance'])
f_importance.sort_values(by='Importance',ascending=False,inplace=True)
f_importance

"""### Using XGBoost"""

clf = xgboost.XGBClassifier()
# Создание экземпляра классификатора градиентного бустинга

clf.fit(X_train, y_train)
# Подгонка модели

# XGBoost classifier accuracy
y_preds = clf.predict(X_test)
# Предсказание данных

print(f"Accuracy : {accuracy_score(y_test, y_preds)*100}%")
# Показатель точности

"""### XGBoost Feature Importance"""

f_importance = pd.DataFrame(clf.feature_importances_*100,index=all_features,columns=['Importance'])
f_importance.sort_values(by='Importance',ascending=False,inplace=True)
f_importance

# saving trained model
filename = '../models/diabetes.sav'
pickle.dump(rfc, open(filename, 'wb'))