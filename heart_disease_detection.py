# -*- coding: utf-8 -*-
"""Heart Disease Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SbasfYR6BQWNERWxNDjMRgO-C87vAKNQ
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns

import pickle
import xgboost

import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif

"""### Reading our data"""

df = pd.read_csv('heart.csv')
df.head()

df.info()

"""### Dropping dome rows

#### thal column is very unbalanced that's why we are dropping thal=0 instances which are just 2
"""

df['thal'].value_counts()

rowstodrop = df[df['thal']==0]
df.drop(index=rowstodrop.index,axis=0,inplace=True)

df['thal'].value_counts()

"""### Creating Features and Target dataframes"""

X = df.drop('target',axis=1)
y = df['target']

X.shape, y.shape

"""### USING CORRELATION MATRIX"""

corr_mat = X.corr()
plt.figure(figsize=(11, 11))
g = sns.heatmap(corr_mat, annot=True, cmap=sns.diverging_palette(20, 220, n=200))

"""### Select K best using Chi^2 test

#### USING FEATURE IMPORTANCE - This technique gives you a score for each feature of your data,the higher the score more relevant it is
"""

ordered_rank_features = SelectKBest(score_func=chi2, k=13)
# Выбор k (13) лучших признаков на основе значения хи-квадрат - функции, которая
# “отсеивает” функции, которые с наибольшей вероятностью не зависят от класса и, следовательно, не имеют отношения к классификации

ordered_feature = ordered_rank_features.fit(X, y)
# Выполним подгонку селектора

feat_imp = pd.DataFrame(ordered_feature.scores_,index=X.columns, columns=["Score"])
feat_imp.sort_values(by='Score',inplace=True,ascending=False)
feat_imp

"""### USING INFORMATION GAIN"""

mutual_info = mutual_info_classif(X, y)
# Выполним подгонку классификатора "взаимной" информации
# чем выше значение у признака, тем выше зависимость между ним и значениями вектора целей

mutual_info_df = pd.DataFrame(mutual_info*100, index=X.columns,columns=['Score'])
mutual_info_df.sort_values(by='Score',inplace=True,ascending=False)
mutual_info_df

"""### FINAL SELECTION"""

final_selected_features = ['ca', 'cp', 'exang', 'thal', 'oldpeak', 'thalach','age']
X = X[final_selected_features]
X.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)
# Разделяем набор данных на тренировочных и тестовый

"""### Training RANDOM FOREST"""

rfc = RandomForestClassifier()
# Создание экземпляра класса классификатора случайного леса

rfc.fit(X_train, y_train)
# подгонка классификатора

# random forest classifier accuracy:
y_preds = rfc.predict(X_test)
# предсказание тестовых данных

print("Accuracy : {:.2f}%".format(accuracy_score(y_test, y_preds)*100))
# показатель точности

"""### Training XGBOOST"""

clf = xgboost.XGBClassifier()
# Создание экземпляра класса классификатора случайного леса

clf.fit(X_train, y_train)
# подгонка классификатора

# xgboost classifier accuracy:
y_preds = clf.predict(X_test)
# предсказание тестовых данных

print("Accuracy : {:.2f}%".format(accuracy_score(y_test, y_preds)*100))
# показатель точности

# saving trained model
filename = '../models/heart_disease.pickle.dat'
pickle.dump(rfc, open(filename, 'wb'))