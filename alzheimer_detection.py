# -*- coding: utf-8 -*-
"""Alzheimer Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12i3SlbZomCPbZ0Z3GKbYCjbWL27LZuux
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

from tensorflow.keras import Sequential, Input
from tensorflow.keras.layers import Dense, Dropout, Conv2D, Flatten, SeparableConv2D, BatchNormalization, MaxPool2D
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG

"""### Defining Train and Test Paths"""

BASE_DIR = "Alzheimer_Dataset/"
TRAIN_DIR = BASE_DIR + 'train'
TEST_DIR = BASE_DIR + 'test'

CLASSES = [ 'NonDemented',
            'VeryMildDemented',
            'MildDemented',
            'ModerateDemented']

IMG_SIZE = 176
DIM = (IMG_SIZE, IMG_SIZE)

"""### Generating Data using DataGenerator"""

datagen = IDG(
    rescale = 1./255, 
    brightness_range=[0.8, 1.2], 
    zoom_range=[.99, 1.01], 
    data_format="channels_last", 
    fill_mode='constant',
    horizontal_flip=True
)

train_data_gen = datagen.flow_from_directory(directory=TRAIN_DIR, target_size=DIM, batch_size=6500, shuffle=False)

"""### Retrieving features and labels"""

#получаем данные из итератора ImageDataGenerator
train_data, train_labels = train_data_gen.next()

#Получаем размеры нашего датасета
print(train_data.shape, train_labels.shape)

"""### Splitting Train Test Data"""

#Разделение данных на обучающие, тестовые и проверочные наборы
train_data, val_data, train_labels, val_labels = train_test_split(train_data, 
                                                                  train_labels, 
                                                                  test_size = 0.2, 
                                                                  random_state=42)

def conv_block(filters, act='relu'):
    """Defining a Convolutional NN block for a Sequential CNN model. """
    
    block = Sequential() 
    # Создаем стек слоев, каждый из которых имеет один входной и один выходной вектор (последовательная модель)
    
    block.add(Conv2D(filters, 3, activation=act, padding='same')) 
    #Задаем слой 2D свертки. 
    #Первый аргумент - представляет количество фильтров, на основе которых обучается сверточный уровень, так называемая глубина тензора
    #Второй аргумент - размер ядра фильтра. В данном случае он представляет собой квадратную матрицу 3*3
    #Третий аргумент - функция активации, в нашем случае - RELU. (max(arg, 0))
    #Четвертый аргумент - настройка нулевого отступа, в нашем случае 'same'
    
    block.add(Conv2D(filters, 3, activation=act, padding='same'))
    #Задаем слой 2D свертки. 
    #Первый аргумент - представляет количество фильтров, на основе которых обучается сверточный уровень, так называемая глубина тензора
    #Второй аргумент - размер ядра фильтра. В данном случае он представляет собой квадратную матрицу 3*3
    #Третий аргумент - функция активации, в нашем случае - RELU. (max(arg, 0))
    #Четвертый аргумент - настройка нулевого отступа, в нашем случае 'same'
    
    block.add(BatchNormalization())
    # Данный слой преобразует входные данные таким образом, что среднее значение выходного сигнала равно 0, а стандартное отклонение - 1
    
    block.add(MaxPool2D())
    # Операция выбора максимального значения из соседних, "агрессивное" уменьшение разрешения карты признаков 
    
    
    return block

def dense_block(units, dropout_rate, act='relu'):
    """Defining a Dense NN block for a Sequential CNN model. """
    
    block = Sequential()
    # Создаем стек слоев, каждый из которых имеет один входной и один выходной вектор (последовательная модель)
    
    block.add(Dense(units, activation=act))
    
    # Обычный слой нейронной сети с плотным соединением
    # Первый аргумент -  Положительное число. Размерность выходного пространства. Он определяет, сколько нейронов будет в этом слое
    # Второй аргумент - функция активации. В нашем случае - RELU. (max(arg, 0))
    
    block.add(BatchNormalization())
    # Данный слой преобразует входные данные таким образом, что среднее значение выходного сигнала равно 0, а стандартное отклонение - 1
    
    block.add(Dropout(dropout_rate))
    # Данный слой зануляет случайно выбранные признаки
    # Первый аргумент соответствует доле отбрасываемых признаков
    
    return block

def construct_model(act='relu'):
    """Constructing a Sequential CNN architecture for performing the classification task. """
    
    model = Sequential([
        Input(shape=(*[IMG_SIZE,IMG_SIZE], 3)), 
        # Задаем размер входных данных - 176 * 176 * 3 - ширина изображения, высота изображения, количество цветовых каналов 
        #в каждом пикселе (RGB)
        
        Conv2D(16, 3, activation=act, padding='same'), 
         #Задаем слой 2D свертки. 
         #Первый аргумент - представляет количество фильтров, на основе которых обучается сверточный уровень, так называемая глубина тензора
         #Второй аргумент - размер ядра фильтра. В данном случае он представляет собой квадратную матрицу 3*3
         #Третий аргумент - функция активации, в нашем случае - RELU. (max(arg, 0))
        
        Conv2D(16, 3, activation=act, padding='same'),
         #Задаем слой 2D свертки. 
         #Первый аргумент - представляет количество фильтров, на основе которых обучается сверточный уровень, так называемая глубина тензора
         #Второй аргумент - размер ядра фильтра. В данном случае он представляет собой квадратную матрицу 3*3
         #Третий аргумент - функция активации, в нашем случае - RELU. (max(arg, 0))
        
        MaxPool2D(),
        # Операция выбора максимального значения из соседних, "агрессивное" уменьшение разрешения карты признаков 
        
        conv_block(32),
        # последовательность слоев, соответствующая функции conv_block, на вход подается значение глубины фильтра 32
        
        conv_block(64),
        # последовательность слоев, соответствующая функции conv_block, на вход подается значение глубины фильтра 64
        
        conv_block(128),
        # последовательность слоев, соответствующая функции conv_block, на вход подается значение глубины фильтра 128
        
        Dropout(0.2),
        # Данный слой зануляет случайно выбранные признаки
        # Первый аргумент соответствует доле отбрасываемых признаков
        
        conv_block(256),
        # последовательность слоев, соответствующая функции conv_block, на вход подается значение глубины фильтра 256
        
        Dropout(0.2),
        # Данный слой зануляет случайно выбранные признаки
        # Первый аргумент соответствует доле отбрасываемых признаков
        
        Flatten(),
        # Преобразует входные данные в один большой одномерный массив
        
        dense_block(512, 0.7),
        # последовательность слоев, соответствующая функции dense_block,
        # подает на вход 512 выходных нейронов и долю отбрасываемых признаков 0.7
        
        dense_block(128, 0.5),
        # последовательность слоев, соответствующая функции dense_block,
        # подает на вход 128 выходных нейронов и долю отбрасываемых признаков 0.5
        
        dense_block(64, 0.3),
        # последовательность слоев, соответствующая функции dense_block,
        # подает на вход 64 выходных нейрона и долю отбрасываемых признаков 0.3
        
        Dense(4, activation='softmax')
        # Обычный слой нейронной сети с плотным соединением
        # Первый аргумент -  Положительное число. Размерность выходного пространства. Он определяет, сколько нейронов будет в этом слое
        # Второй аргумент - функция активации. В нашем случае - RELU. (max(arg, 0))
        
    ], name = "cnn_model") # имя модели

    return model

"""### Defining MyCallback class

It says that when on the end of an epoch, the val accuracy crosses 0.99 stop the training.
"""

class MyCallback(tf.keras.callbacks.Callback):
    """Контролирует значение точности. Останавливает тренировку модели по достижении заданного значения"""
    def on_epoch_end(self, epoch, logs={}):
        if logs.get('val_acc') > 0.99:
            print("\nReached accuracy threshold! Terminating training.")
            self.model.stop_training = True
            
my_callback = MyCallback()
CALLBACKS = [my_callback]

model = construct_model() # конструирование модели

METRICS = [tf.keras.metrics.CategoricalAccuracy(name='acc'),
           tf.keras.metrics.AUC(name='auc')] # задание метрик качества модели 
           # CategoticalAccuracy - измерение частоты совпадений предсказанного значения с истинным
           # AUC - аппрроксимирует площадь под кривыми ROC (Receiver operating characteristic;по умолчанию) или PR (Precision Recall)
           # Значения обеих метрик лежат в диапазоне от нуля до одного



model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=METRICS)

# Настраиваем модель для обучения
# Первый аргумент - оптимизатор
# Второй аргумент - задание функции потерь
# Третий аргумент метрики качеста модели

# model.summary()

#Fit the training data to the model and validate it using the validation data
EPOCHS = 50
# Количество эпох
history = model.fit(train_data, train_labels, validation_data=(val_data, val_labels), callbacks=CALLBACKS, epochs=EPOCHS)
# Подгонка модели
model.save('../models/alzheimer_model.h5')
# Сохранение модели

"""### Plot model performance

"""

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(1, len(history.epoch) + 1)

plt.figure(figsize=(15,5))

plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Train Set')
plt.plot(epochs_range, val_acc, label='Val Set')
plt.legend(loc="best")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Train Set')
plt.plot(epochs_range, val_loss, label='Val Set')
plt.legend(loc="best")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')

plt.tight_layout()
plt.show()

"""### Predicting the test data"""

pred_labels = model.predict(test_data)

"""### Plot the confusion matrix to understand the classification in detail"""

pred_ls = np.argmax(pred_labels, axis=1)
test_ls = np.argmax(test_labels, axis=1)

conf_arr = confusion_matrix(test_ls, pred_ls)

plt.figure(figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')

ax = sns.heatmap(conf_arr, cmap='Greens', annot=True, fmt='d', xticklabels=CLASSES, yticklabels=CLASSES)

plt.title('Alzheimer\'s Disease Diagnosis')
plt.xlabel('Prediction')
plt.ylabel('Truth')
plt.show(ax)